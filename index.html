<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>HALO: Long Horizon Latent Action Learning</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <nav>
    <a href="#abstract">Abstract</a>
    <a href="#architecture">Architecture</a>
    <a href="#contributions">Contributions</a>
    <a href="#results">Results</a>
    <a href="#tasks">Tasks</a>
    <a href="#video">Video</a>
  </nav>

  <header>
    <h1>
      <span style="color:red">H</span><span style="color:green">A</span><span style="color:blue">L</span><span style="color:orange">O</span>: Long Horizon Latent Action Learning for General Robot Manipulation
    </h1>
    <p><em>NeurIPS 2025 Submission</em></p>
  </header>

  <section id="abstract">
    <h2>Abstract</h2>
    <p>
      Robotic manipulation often requires understanding long-horizon tasks guided by visual observations and language instructions.
      However, most existing Vision-Language-Action (VLA) models focus primarily on short-horizon tasks and overlook the rich historical video context,
      limiting their ability to perform complex, multi-step tasks.
      Moreover, these models often suffer from weak alignment between pre-trained vision-language embeddings and robotic actions,
      which hinders the effective extraction of action-relevant priors from visual input and leads to inaccurate action generation.
      In this paper, we propose a novel Long <strong style="color:red">H</strong>orizon Latent <strong style="color:green">A</strong>ction <strong style="color:blue">L</strong>earning framework for general r<strong style="color:orange">o</strong>bot manipulation,
      <strong>HALO</strong>, which enables robots to perform multi-step tasks by integrating long-term visual observations, multi-view camera images,
      and natural language instructions.
    </p>
  </section>

  <section id="architecture">
    <h2>Model Architecture</h2>
    <img src="assets/images/overview.JPG" alt="HALO Model Architecture" width="100%">
    <img src = "assets/images/attention.JPG" alt="Attention Mechanism" width="100%">
    <p>HALO integrates a pretrained VLM, a state-aware latent re-representation module, and an action expert to generate multi-step robot actions.</p>
  </section>

  <section id="contributions">
    <h2>Key Contributions</h2>
    <ul>
      <li>Supports long-horizon visual-linguistic context</li>
      <li>State-aware latent re-representation for cross-modal alignment</li>
      <li>10B parameter scale trained on 1M+ robot episodes</li>
      <li>Outperforms RT-2-X and pi0 in both simulation and real-world</li>
    </ul>
  </section>

  <section id="results">
    <h2>Experimental Results</h2>
    <table border="1">
      <caption>Success Rates on SIMPLER (Google Robot)</caption>
      <thead>
        <tr><th>Method</th><th>Single-Step Avg</th><th>Multi-Step Avg</th></tr>
      </thead>
      <tbody>
        <tr><td>RT-2-X</td><td>78.7%</td><td>3.7%</td></tr>
        <tr><td>pi0</td><td>87.3%</td><td>16.0%</td></tr>
        <tr><td><strong>HALO (Ours)</strong></td><td><strong>94.3%</strong></td><td><strong>19.4%</strong></td></tr>
      </tbody>
    </table>
    <img src="assets/images/sim_results.png" alt="Comparison Chart" width="100%">
  </section>

  <section id="tasks">
    <h2>Task Visualization</h2>
    <img src="assets/images/real_word.JPG" alt="Heat the Food Sequence" width="100%">
    <p>Example long-horizon task: placing a pot in the oven, closing it, setting timer, reopening, and returning pot to table.</p>
  </section>

  <section id="video">
    <h2>Demo Video</h2>
    <video controls width="640">
      <source src="assets/video/demo.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <!-- For YouTube version:
    <iframe width="560" height="315" src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allowfullscreen></iframe>
    -->
  </section>
  <footer>
    <p><a href="halo.pdf">Download Paper (PDF)</a> | <a href="https://github.com/NsurrenderX/gcr_lerobot_2">GitHub Repo</a></p>
  </footer>
</body>
</html>