<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>HALO</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
<section class="hero">
  <div class="hero-body">
    <div class="container has-text-centered">

      <h1 class="title is-1 publication-title">
                <span style="color:red">H</span><span style="color:green">A</span><span style="color:blue">L</span><span style="color:orange">O</span>
      </h1>
      <h2 class="title is-3 publication-subtitle">
        Long <strong style="color:red">H</strong>orizon 
      Latent <strong style="color:green">A</strong>ction <strong style="color:blue">L</strong>earning framework for general r<strong style="color:orange">O</strong>bot manipulation
      </h2>

      <div class="authors">
        <span class="author">Author1<sup>*,1,2</sup></span>,
        <span class="author">Author2<sup>*,1,2</sup></span>,
        </div>

      <div class="affiliations">
        <span class="affiliation"><sup>1</sup>Your First Institution</span>
        <span class="affiliation"><sup>2</sup>Your Second Institution</span>
        <span class="affiliation"><sup>3</sup>Your Third Institution</span>
      </div>

      <div class="contribution-notes">
          <p><sup>*</sup>Equal Contribution &nbsp;&nbsp;&nbsp;&nbsp; <sup>â€ </sup>Corresponding Author</p>
      </div>

      <div class="acceptance-line">
        <p>Accepted by 
          <!-- <strong>ICML 2025</strong></p> -->
      </div>

      <div class="links-container">
        <a href="halo.pdf" class="button">ðŸ“„ Paper</a>
        <a href="https://github.com/NsurrenderX/gcr_lerobot_2" class="button">ðŸ’» Code</a>
        <a href="#" class="button">ðŸ“– arXiv</a>
      </div>

    </div>
  </div>
</section>

  <section class="content">
    <h2>Abstract</h2>
    <p>
      Robotic manipulation often requires understanding long-horizon tasks guided by visual observations and language instructions.
      However, most existing Vision-Language-Action (VLA) models focus primarily on short-horizon tasks and overlook the rich historical video context,
      limiting their ability to perform complex, multi-step tasks.
      Moreover, these models often suffer from weak alignment between pre-trained vision-language embeddings and robotic actions,
      which hinders the effective extraction of action-relevant priors from visual 
      input and leads to inaccurate action generation.
      In this paper, we propose a novel Long <strong style="color:red">H</strong>orizon 
      Latent <strong style="color:green">A</strong>ction <strong style="color:blue">L</strong>earning framework for general r<strong style="color:orange">O</strong>bot manipulation,
          <strong style="color:red">H</strong><strong style="color:green">A</strong><strong style="color:blue">L</strong><strong style="color:orange">O</strong>, which enables robots to perform multi-step tasks by integrating long-term visual observations, multi-view camera images,
      and natural language instructions.
    </p>

    <h2>Model Architecture</h2>
    <img src="assets/images/overview.JPG" alt="HALO Model Architecture" class="fullwidth">
    <p>
      HALO integrates a pretrained VLM, a state-aware latent re-representation module, and an action expert to generate multi-step robot actions.
    </p>
    <img src="assets/images/attention.JPG" alt="Attention Mechanism" class="fullwidth">
    <p>
      The attention mechanism allows HALO to focus on relevant parts of the visual input while generating actions.
    </p>

    <h2>Key Contributions</h2>
    <ul>
      <li>Supports long-horizon visual-linguistic context</li>
      <li>State-aware latent re-representation for cross-modal alignment</li>
      <li>10B parameter scale trained on 1M+ robot episodes</li>
      <li>Outperforms RT-2-X and pi0 in both simulation and real-world</li>
    </ul>

    <h2>Experimental Results</h2>
    <img src="assets/images/simpler results.JPG" alt="Comparison Chart" class="fullwidth">
    <img src="assets/images/libreo results.JPG" alt="table of libreo results" class="fullwidth">
        <p>
      HALO outperforms existing models like RT-2-X and pi0 in both simulation and real-world tasks, demonstrating superior long-horizon task performance.
    </p>
    <h2>Task Visualization</h2>
    <img src="assets/images/real_word.JPG" alt="Heat the Food Sequence" class="fullwidth">
    <p>
      Example long-horizon task: <br>
      (a) Open the oven, pick up the bowl, pour the bowl of food into pot, put the pot in the oven and close the oven.
      <br>
      (b) Put the pot in the oven, close the oven, set the timer, open the oven and put the potbackon the table.
    </p>

    <h2>Demo Video</h2>
    <video controls class="video">
      <source src="assets/video/demo.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <p style="text-align: center;">
      <a href="halo.pdf">Download Paper (PDF)</a> | <a href="https://github.com/HALO-ROBOT/HALO-ROBOT.github.io">GitHub Repo</a>
    </p>
  </section>
</body>
</html>