<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>HALO</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <section class="hero">
    <h1>
      <span style="color:red">H</span><span style="color:green">A</span><span style="color:blue">L</span><span style="color:orange">O</span>
    </h1>
    <h2>Long <strong style="color: red;">H</strong>orizon <strong style="color: green;">L</strong>atent 
      <strong style="color: blue;">A</strong>ction Learning for General R<strong style="color: orange;">O</strong>bot Manipulation</h2>
    <p><em>Authors</em></p>
  </section>

  <section class="content">
    <h2>Abstract</h2>
    <p>
      Robotic manipulation often requires understanding long-horizon tasks guided by visual observations and language instructions.
      However, most existing Vision-Language-Action (VLA) models focus primarily on short-horizon tasks and overlook the rich historical video context,
      limiting their ability to perform complex, multi-step tasks.
      Moreover, these models often suffer from weak alignment between pre-trained vision-language embeddings and robotic actions,
      which hinders the effective extraction of action-relevant priors from visual 
      input and leads to inaccurate action generation.
      In this paper, we propose a novel Long <strong style="color:red">H</strong>orizon 
      Latent <strong style="color:green">A</strong>ction <strong style="color:blue">L</strong>earning framework for general r<strong style="color:orange">O</strong>bot manipulation,
          <strong style="color:red">H</strong><strong style="color:green">A</strong><strong style="color:blue">L</strong><strong style="color:orange">O</strong>, which enables robots to perform multi-step tasks by integrating long-term visual observations, multi-view camera images,
      and natural language instructions.
    </p>

    <h2>Model Architecture</h2>
    <img src="assets/images/overview.JPG" alt="HALO Model Architecture" class="fullwidth">
    <p>
      HALO integrates a pretrained VLM, a state-aware latent re-representation module, and an action expert to generate multi-step robot actions.
    </p>
    <img src="assets/images/attention.JPG" alt="Attention Mechanism" class="fullwidth">
    <p>
      The attention mechanism allows HALO to focus on relevant parts of the visual input while generating actions.
    </p>

    <h2>Key Contributions</h2>
    <ul>
      <li>Supports long-horizon visual-linguistic context</li>
      <li>State-aware latent re-representation for cross-modal alignment</li>
      <li>10B parameter scale trained on 1M+ robot episodes</li>
      <li>Outperforms RT-2-X and pi0 in both simulation and real-world</li>
    </ul>

    <h2>Experimental Results</h2>
    <img src="assets/images/simpler results.JPG" alt="Comparison Chart" class="fullwidth">
    <img src="assets/images/libreo results.JPG" alt="table of libreo results" class="fullwidth">
        <p>
      HALO outperforms existing models like RT-2-X and pi0 in both simulation and real-world tasks, demonstrating superior long-horizon task performance.
    </p>
    <h2>Task Visualization</h2>
    <img src="assets/images/real_word.JPG" alt="Heat the Food Sequence" class="fullwidth">
    <p>
      Example long-horizon task: <br>
      (a) Open the oven, pick up the bowl, pour the bowl of food into pot, put the pot in the oven and close the oven.
      <br>
      (b) Put the pot in the oven, close the oven, set the timer, open the oven and put the potbackon the table.
    </p>

    <h2>Demo Video</h2>
    <video controls class="video">
      <source src="assets/video/demo.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <p style="text-align: center;">
      <a href="halo.pdf">Download Paper (PDF)</a> | <a href="https://github.com/HALO-ROBOT/HALO-ROBOT.github.io">GitHub Repo</a>
    </p>
  </section>
</body>
</html>